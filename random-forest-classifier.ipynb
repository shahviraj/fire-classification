{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import cv2 as cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as scio\n",
    "import h5py\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import MiniBatchDictionaryLearning\n",
    "from time import time\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data\n",
    "comb_data = h5py.File('../Aditya_data/combustion_img_13.mat','r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = comb_data['train_set_x'][()]\n",
    "y_train = comb_data['train_set_y'][()]\n",
    "y_train = np.ravel(y_train)\n",
    "X_test = comb_data['test_set_x'][()]\n",
    "y_test = comb_data['test_set_y'][()]\n",
    "y_test = np.ravel(y_test)\n",
    "X_valid = comb_data['valid_set_x'][()]\n",
    "y_valid = comb_data['valid_set_y'][()]\n",
    "y_valid = np.ravel(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_final = np.zeros([54000,1250])\n",
    "n=len(X_train_final[:,1])\n",
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/viraj/venvs/tensorflow/lib/python2.7/site-packages/skimage/feature/_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    }
   ],
   "source": [
    "#Feature extraction using Histogram of Gradients\n",
    "#Training data\n",
    "for i in range(0,n):\n",
    "    temp_image = X_train[:,i]\n",
    "    \n",
    "    temp_image = np.reshape(temp_image,[250,100])\n",
    "    temp_image = temp_image.T\n",
    "    temp_fd = hog(temp_image, orientations=5, pixels_per_cell=(10, 10),\n",
    "                    cells_per_block=(1, 1))\n",
    "    \n",
    "    X_train_final[i,:] = temp_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract features of test data using Histogram of Gradients\n",
    "X_test_final = np.zeros([18000,1250])\n",
    "b=len(X_test_final[:,1])\n",
    "for i in range(0,b):\n",
    "    temp_image = X_test[:,i]\n",
    "    \n",
    "    temp_image = np.reshape(temp_image,[250,100])\n",
    "    temp_image = temp_image.T\n",
    "    temp_fd = hog(temp_image, orientations=5, pixels_per_cell=(10, 10),\n",
    "                    cells_per_block=(1, 1))\n",
    "    \n",
    "    X_test_final[i,:] = temp_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract features of validation data using Histogram of Gradients\n",
    "X_valid_final = np.zeros([9000,1250])\n",
    "b=len(X_valid_final[:,1])\n",
    "for i in range(0,b):\n",
    "    temp_image = X_valid[:,i]\n",
    "    \n",
    "    temp_image = np.reshape(temp_image,[250,100])\n",
    "    temp_image = temp_image.T\n",
    "    temp_fd = hog(temp_image, orientations=5, pixels_per_cell=(10, 10),\n",
    "                    cells_per_block=(1, 1))\n",
    "    \n",
    "    X_valid_final[i,:] = temp_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 54000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning the dictionary...\n",
      "done in 146.95s.\n"
     ]
    }
   ],
   "source": [
    "#Feature extraction using Dictionary Learning\n",
    "#Training data\n",
    "print('Learning the dictionary...')\n",
    "t0 = time()\n",
    "dico = MiniBatchDictionaryLearning(n_components=10, alpha=1, n_iter=100)\n",
    "X_train_dict = dico.fit_transform(X_train.T)\n",
    "np.shape(X_train_dict)\n",
    "dt = time() - t0\n",
    "print('done in %.2fs.' % dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Express test data in terms of (Dictionary) learned features\n",
    "X_test_dict = dico.transform(X_test.T)\n",
    "np.shape(X_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Express validation data in terms of (Dictionary) learned features\n",
    "X_valid_dict = dico.transform(X_valid.T)\n",
    "np.shape(X_valid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=2, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random forest classifier for HOG features # n_features = 1250\n",
    "num_features = \"auto\" #default option \"auto\" = sqrt(n_features); \"log2\" = log2(n_features); None = n_features \n",
    "clf = RandomForestClassifier(n_estimators=2, max_features=32, max_depth=2, random_state=0)\n",
    "clf.fit(X_train_final, y_train)\n",
    "\n",
    "#Random forest classifier for dictionary features # n_features = 10\n",
    "num_features_d = \"auto\" #default option \"auto\" = sqrt(n_features); \"log2\" = log2(n_features); None = n_features \n",
    "clfd = RandomForestClassifier(n_estimators=2, max_features=num_features_d, max_depth=4, random_state=0)\n",
    "clfd.fit(X_train_dict, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features=1250, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=6, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random forest classifier for HOG features # n_features = 1250\n",
    "num_features = \"auto\" #default option \"auto\" = sqrt(n_features); \"log2\" = log2(n_features); None = n_features \n",
    "clf = RandomForestClassifier(n_estimators=6, max_features=1250, max_depth=4, random_state=0)\n",
    "clf.fit(X_train_final, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifier using HOG features is... 0.971388888889\n"
     ]
    }
   ],
   "source": [
    "#Test classification accuracy using Random Forest Classifier for HOG features\n",
    "\n",
    "y_test_predict=clf.predict(X_test_final)\n",
    "acc = accuracy_score(y_test, y_test_predict)\n",
    "print('Accuracy of Random Forest Classifier using HOG features is...',acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=4, max_features=10, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=6, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Random forest classifier for dictionary features # n_features = 10\n",
    "num_features_d = \"auto\" #default option \"auto\" = sqrt(n_features); \"log2\" = log2(n_features); None = n_features \n",
    "clfd = RandomForestClassifier(n_estimators=6, max_features=10, max_depth=4, random_state=0)\n",
    "clfd.fit(X_train_dict, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Random Forest Classifier using Dictionary learned features is... 0.913055555556\n"
     ]
    }
   ],
   "source": [
    "#Test classification accuracy using Random Forest Classifier for dictionary features\n",
    "\n",
    "y_test_predictd=clfd.predict(X_test_dict)\n",
    "acc_d = accuracy_score(y_test, y_test_predictd)\n",
    "print('Accuracy of Random Forest Classifier using Dictionary learned features is...',acc_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree Classifier using HOG features is... 0.928\n",
      "Accuracy of Decision Tree Classifier using Dictionary learned features is... 0.9125\n"
     ]
    }
   ],
   "source": [
    "##Compare the performance to Decision Tree, Extra Trees, AdaBoost, Voting and Gradient Tree Boosting.\n",
    "\n",
    "#Decision Tree\n",
    "clf_dt = tree.DecisionTreeClassifier(max_depth=4, max_features=30)\n",
    "clf_dt.fit(X_train_final, y_train)\n",
    "y_test_predict_dectree=clf_dt.predict(X_test_final)\n",
    "acc_dt = accuracy_score(y_test, y_test_predict_dectree)\n",
    "print('Accuracy of Decision Tree Classifier using HOG features is...',acc_dt)\n",
    "clf_dt_d = tree.DecisionTreeClassifier(max_depth=4, max_features=9)\n",
    "clf_dt_d.fit(X_train_dict, y_train)\n",
    "y_test_predict_dectree_d=clf_dt_d.predict(X_test_dict)\n",
    "acc_dt_d = accuracy_score(y_test, y_test_predict_dectree_d)\n",
    "print('Accuracy of Decision Tree Classifier using Dictionary learned features is...',acc_dt_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Extra Trees Classifier using HOG features is... 0.889\n",
      "Accuracy of Extra Trees Classifier using Dictionary learned features is... 0.939222222222\n"
     ]
    }
   ],
   "source": [
    "#Extra Trees\n",
    "extclf= ExtraTreesClassifier(n_estimators=250, max_depth=4, max_features=30, random_state=0)\n",
    "extclf.fit(X_train_final, y_train)\n",
    "y_test_predict_exttree=extclf.predict(X_test_final)\n",
    "acc_et = accuracy_score(y_test, y_test_predict_exttree)\n",
    "print('Accuracy of Extra Trees Classifier using HOG features is...',acc_et)\n",
    "extclf_d= ExtraTreesClassifier(n_estimators=10, max_depth=4, max_features=9, random_state=0)\n",
    "extclf_d.fit(X_train_dict, y_train)\n",
    "y_test_predict_exttree_d=extclf_d.predict(X_test_dict)\n",
    "acc_et_d = accuracy_score(y_test, y_test_predict_exttree_d)\n",
    "print('Accuracy of Extra Trees Classifier using Dictionary learned features is...',acc_et_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Gradient Boosting Classifier using HOG features is... 0.996111111111\n",
      "Accuracy of Gradient Boosting Classifier using Dictionary learned features is... 0.978\n"
     ]
    }
   ],
   "source": [
    "#Gradient Boosting Classifier\n",
    "clfgb = GradientBoostingClassifier(n_estimators=30, learning_rate=1.0, max_depth=4, max_features=3, random_state=0)\n",
    "clfgb.fit(X_train_final, y_train)\n",
    "y_test_predict_gradboost=clfgb.predict(X_test_final)\n",
    "acc_gb = accuracy_score(y_test, y_test_predict_gradboost)\n",
    "print('Accuracy of Gradient Boosting Classifier using HOG features is...',acc_gb)\n",
    "clfgb_d = GradientBoostingClassifier(n_estimators=9, learning_rate=1.0, max_depth=4, max_features=3, random_state=0)\n",
    "clfgb_d.fit(X_train_dict, y_train)\n",
    "y_test_predict_gradboostd=clfgb_d.predict(X_test_dict)\n",
    "acc_gb_d = accuracy_score(y_test, y_test_predict_gradboostd)\n",
    "print('Accuracy of Gradient Boosting Classifier using Dictionary learned features is...',acc_gb_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of AdaBoost Classifier using HOG features is... 0.998055555556\n",
      "Accuracy of AdaBoost Classifier using Dictionary learned features is... 0.960055555556\n"
     ]
    }
   ],
   "source": [
    "#AdaBoost Classifier\n",
    "clfab = AdaBoostClassifier()\n",
    "clfab.fit(X_train_final, y_train)\n",
    "y_test_predict_adboost=clfab.predict(X_test_final)\n",
    "acc_ab = accuracy_score(y_test, y_test_predict_adboost)\n",
    "print('Accuracy of AdaBoost Classifier using HOG features is...',acc_ab)\n",
    "clfab_d = AdaBoostClassifier()\n",
    "clfab_d.fit(X_train_dict, y_train)\n",
    "y_test_predict_adboostd=clfab_d.predict(X_test_dict)\n",
    "acc_ab_d = accuracy_score(y_test, y_test_predict_adboostd)\n",
    "print('Accuracy of AdaBoost Classifier using Dictionary learned features is...',acc_ab_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Voting Classifier using HOG features is... 0.997166666667\n",
      "Accuracy of Voting Classifier using Dictionary learned features is... 0.990833333333\n"
     ]
    }
   ],
   "source": [
    "#Voting Classifier\n",
    "clfrf = RandomForestClassifier(random_state=1)\n",
    "clfvote = VotingClassifier(estimators=[('rf', clfrf)], voting='hard')\n",
    "clfvote.fit(X_train_final, y_train)\n",
    "y_test_predict_vote=clfvote.predict(X_test_final)\n",
    "acc_v = accuracy_score(y_test, y_test_predict_vote)\n",
    "print('Accuracy of Voting Classifier using HOG features is...',acc_v)\n",
    "clfrfd = RandomForestClassifier(random_state=1)\n",
    "clfvoted = VotingClassifier(estimators=[('rf', clfrfd)], voting='hard')\n",
    "clfvoted.fit(X_train_dict, y_train)\n",
    "y_test_predict_voted=clfvoted.predict(X_test_dict)\n",
    "acc_vd = accuracy_score(y_test, y_test_predict_voted)\n",
    "print('Accuracy of Voting Classifier using Dictionary learned features is...',acc_vd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging original training and validation data for HOG features...\n",
      "Merging original training and validation data for Dictionary learned features...\n"
     ]
    }
   ],
   "source": [
    "#KFold Cross validation\n",
    "\n",
    "print('Merging original training and validation data for HOG features...')\n",
    "X_full = np.append(X_train_final,X_valid_final,axis=0)\n",
    "\n",
    "print('Merging original training and validation data for Dictionary learned features...')\n",
    "X_full_d = np.append(X_train_dict,X_valid_dict,axis=0)\n",
    "\n",
    "y_full = np.append(y_train,y_valid)\n",
    "\n",
    "kf = KFold(n_splits=2)  #n_splits can be increased, but will result in higher running time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63000, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_full_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [27000 27001 27002 ..., 53997 53998 53999] TEST: [    0     1     2 ..., 26997 26998 26999]\n",
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unbound method get_params() must be called with VotingClassifier instance as first argument (got nothing instead)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-b0239488e184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         clf_gs = GridSearchCV(clf_considered,# this handle can be changed for any other classifier\n\u001b[1;32m     35\u001b[0m                               tuned_parameters_vc,  scoring='%s_macro' % score)\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mclf_gs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_gs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_gs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#HOG features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/viraj/venvs/tensorflow/lib/python2.7/site-packages/sklearn/model_selection/_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    622\u001b[0m                                      n_candidates * n_splits))\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m         \u001b[0mbase_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/viraj/venvs/tensorflow/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     57\u001b[0m                             % (repr(estimator), type(estimator)))\n\u001b[1;32m     58\u001b[0m     \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0mnew_object_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_object_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mnew_object_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unbound method get_params() must be called with VotingClassifier instance as first argument (got nothing instead)"
     ]
    }
   ],
   "source": [
    "#Grid search on K=2 fold cross validation dataset\n",
    "for train_index, test_index in kf.split(X_train_final):\n",
    "    \n",
    "    #Generate new cross-validation dataset\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    #reduced dataset with HOG features\n",
    "    X_train_gs, X_test_gs = X_full[train_index], X_full[test_index]\n",
    "    #reduced dataset with Dictionary learned features\n",
    "    X_train_gs_d, X_test_gs_d = X_full_d[train_index], X_full_d[test_index]    \n",
    "    y_train_gs, y_test_gs = y_full[train_index], y_full[test_index]\n",
    "    \n",
    "    #Get optimal hyper-parameters for this CV dataset\n",
    "    \n",
    "    tuned_parameters_rf = [{\"n_estimators\": [2, 3], \"max_depth\": [2, 4], \"max_features\": [10, 30]}] #add max_features also\n",
    "    tuned_parameters_dt = [{\"max_depth\": [2, 4], \"max_features\": [10, 30]}] \n",
    "    tuned_parameters_et = [{\"max_depth\": [2, 4], \"max_features\": [10, 30]}]\n",
    "    tuned_parameters_gb = [{\"n_estimators\": [2, 3], \"max_depth\": [2, 4], \"max_features\": [10, 30]}]\n",
    "    tuned_parameters_ab = [{\"n_estimators\": [2, 3], }]\n",
    "    tuned_parameters_vc = [{\"estimators\": [2, 3]}]\n",
    "    \n",
    "    #clf_considered = RandomForestClassifier()\n",
    "    #clf_considered = tree.DecisionTreeClassifier()\n",
    "    #clf_considered = ExtraTreesClassifier()\n",
    "    #clf_considered = GradientBoostingClassifier()\n",
    "    #clf_considered = AdaBoostClassifier()\n",
    "    clf_considered = VotingClassifier\n",
    "   \n",
    "   \n",
    "    scores = ['precision', 'recall']\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "        clf_gs = GridSearchCV(clf_considered,# this handle can be changed for any other classifier\n",
    "                              tuned_parameters_vc,  scoring='%s_macro' % score)\n",
    "        clf_gs.fit(X_train_gs, np.ravel(y_train_gs))\n",
    "\n",
    "        #HOG features\n",
    "        print(\"Best parameters set found on development set, using HOG features:\")\n",
    "        print()\n",
    "        print(clf_gs.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set, using HOG features:\")\n",
    "        print()\n",
    "        means = clf_gs.cv_results_['mean_test_score']\n",
    "        stds = clf_gs.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf_gs.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set, using HOG features.\")\n",
    "        print(\"The scores are computed on the full evaluation set, using HOG features.\")\n",
    "        print()\n",
    "        y_true, y_pred = y_test_gs, clf_gs.predict(X_test_gs)\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        print()\n",
    "    \n",
    "        \n",
    "        \n",
    "    #Get optimal hyper-parameters for this CV dataset\n",
    "    tuned_parameters_rf_dl = [{\"n_estimators\": [2, 3], \"max_depth\": [2, 4], \"max_features\": [4, 9]}] #add max_features also\n",
    "    tuned_parameters_dt_dl = [{\"max_depth\": [2, 4], \"max_features\": [4, 9]}] \n",
    "    tuned_parameters_et_dl = [{\"max_depth\": [2, 4], \"max_features\": [4, 9]}]\n",
    "    tuned_parameters_gb_dl = [{\"n_estimators\": [2, 3], \"max_depth\": [2, 4], \"max_features\": [4, 9]}]\n",
    "    tuned_parameters_ab_dl = [{\"n_estimators\": [2, 3]}]\n",
    "    tuned_parameters_vc_dl = [{\"estimators\": [2, 3]}]\n",
    "    scores = ['precision', 'recall']\n",
    "    for score in scores:\n",
    "        print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "        print()\n",
    "\n",
    "      \n",
    "        #Dictionary learned features\n",
    "        clf_gs_d = GridSearchCV(clf_considered,# this handle can be changed for any other classifier\n",
    "                              tuned_parameters_vc_dl,  scoring='%s_macro' % score)\n",
    "        clf_gs_d.fit(X_train_gs_d, y_train_gs)\n",
    "        print(\"Best parameters set found on development set, using Dictionary learned features:\")\n",
    "        print()\n",
    "        print(clf_gs_d.best_params_)\n",
    "        print()\n",
    "        print(\"Grid scores on development set, using Dictionary learned features:\")\n",
    "        print()\n",
    "        means = clf_gs_d.cv_results_['mean_test_score']\n",
    "        stds = clf_gs_d.cv_results_['std_test_score']\n",
    "        for mean, std, params in zip(means, stds, clf_gs_d.cv_results_['params']):\n",
    "            print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                  % (mean, std * 2, params))\n",
    "        print()\n",
    "\n",
    "        print(\"Detailed classification report:\")\n",
    "        print()\n",
    "        print(\"The model is trained on the full development set, using Dictionary learned features.\")\n",
    "        print(\"The scores are computed on the full evaluation set, using Dictionary learned features.\")\n",
    "        print()\n",
    "        y_true_d, y_pred_d = y_test_gs, clf_gs_d.predict(X_test_gs_d)\n",
    "        print(classification_report(y_true_d, y_pred_d))\n",
    "        print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
